import re
import asyncio
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, parse_qs

# /verify ‡¶ï‡¶Æ‡¶æ‡¶®‡ßç‡¶°
async def verify_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text(
        "*Verification Link Finder 2025*\n\n"
        "‡¶™‡ßá‡¶∏‡ßç‡¶ü ‡¶ï‡¶∞‡ßã ‡¶è‡¶á ‡¶´‡¶∞‡¶Æ‡ßç‡¶Ø‡¶æ‡¶ü‡ßá:\n"
        "`email|password|cookies|user_id`\n\n"
        "‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£:\n"
        "`example@outlook.com|pass123|__Host-...; csrftoken=...|123456789`",
        parse_mode="Markdown"
    )

# ‡¶Æ‡ßÇ‡¶≤ ‡¶™‡ßç‡¶∞‡¶∏‡ßá‡¶∏‡¶ø‡¶Ç ‡¶´‡¶æ‡¶Ç‡¶∂‡¶® (‡¶´‡¶ø‡¶ï‡ßç‡¶∏‡¶° ‡¶≠‡¶æ‡¶∞‡ßç‡¶∏‡¶®)
async def process_account(update: Update, context: ContextTypes.DEFAULT_TYPE):
    text = update.message.text.strip()
    
    if text.count("|") != 3:
        await update.message.reply_text("‚ùå ‡¶≠‡ßÅ‡¶≤ ‡¶´‡¶∞‡¶Æ‡ßç‡¶Ø‡¶æ‡¶ü!\n‡¶∏‡¶†‡¶ø‡¶ï ‡¶´‡¶∞‡¶Æ‡ßç‡¶Ø‡¶æ‡¶ü: `email|pass|cookies|id`")
        return

    try:
        email, password, cookies_str, user_id = text.split("|", 3)
    except:
        await update.message.reply_text("‚ùå ‡¶∏‡ßç‡¶™‡ßç‡¶≤‡¶ø‡¶ü ‡¶ï‡¶∞‡¶§‡ßá ‡¶∏‡¶Æ‡¶∏‡ßç‡¶Ø‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá")
        return

    await update.message.reply_text("üîÑ ‡¶ï‡ßÅ‡¶ï‡¶ø‡¶ú ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá ‡¶≤‡¶ó‡¶á‡¶® ‡¶ï‡¶∞‡¶æ ‡¶π‡¶ö‡ßç‡¶õ‡ßá... (10-20 ‡¶∏‡ßá‡¶ï‡ßá‡¶®‡ßç‡¶°)")

    # ‡¶ï‡ßÅ‡¶ï‡¶ø‡¶ú ‡¶™‡¶æ‡¶∞‡ßç‡¶∏
    cookie_dict = {}
    for part in cookies_str.split(";"):
        part = part.strip()
        if "=" in part:
            k, v = part.split("=", 1)
            cookie_dict[k] = v

    session = requests.Session()
    session.headers.update({
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
        "Accept-Language": "en-US,en;q=0.5",
        "Accept-Encoding": "gzip, deflate, br",
        "Connection": "keep-alive",
        "Upgrade-Insecure-Requests": "1",
    })

    try:
        # ‡¶è‡¶á URL-‡¶ü‡¶æ‡¶á ‡¶è‡¶ñ‡¶®‡ßã ‡¶ï‡ßÅ‡¶ï‡¶ø‡¶ú ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá ‡¶∏‡¶∞‡¶æ‡¶∏‡¶∞‡¶ø ‡¶ï‡¶æ‡¶ú ‡¶ï‡¶∞‡ßá (2025 ‡¶™‡¶∞‡ßç‡¶Ø‡¶®‡ßç‡¶§ ‡¶ü‡ßá‡¶∏‡ßç‡¶ü‡ßá‡¶°)
        r = session.get(
            "https://outlook.live.com/owa/",
            cookies=cookie_dict,
            allow_redirects=True,
            timeout=25
        )

        # ‡¶Ø‡¶¶‡¶ø ‡¶≤‡¶ó‡¶á‡¶® ‡¶™‡ßá‡¶ú‡ßá ‡¶∞‡¶ø‡¶°‡¶æ‡¶á‡¶∞‡ßá‡¶ï‡ßç‡¶ü ‡¶π‡¶Ø‡¶º
        if "login.microsoftonline.com" in r.url or "Sign in" in r.text or r.status_code != 200:
            await update.message.reply_text("‚ùå ‡¶ï‡ßÅ‡¶ï‡¶ø‡¶ú ‡¶è‡¶ï‡ßç‡¶∏‡¶™‡¶æ‡¶Ø‡¶º‡¶æ‡¶∞‡ßç‡¶° ‡¶Ö‡¶•‡¶¨‡¶æ ‡¶á‡¶®‡¶≠‡ßç‡¶Ø‡¶æ‡¶≤‡¶ø‡¶°")
            return

        # Inbox-‡¶è ‡¶Ø‡¶æ‡¶ì‡¶Ø‡¶º‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø OWA-‡¶∞ API endpoint
        inbox_url = "https://outlook.live.com/mail/inbox"
        r2 = session.get(inbox_url, cookies=cookie_dict, timeout=25)

        if r2.status_code != 200:
            await update.message.reply_text("‚ùå Inbox ‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡¶§‡ßá ‡¶´‡ßá‡¶á‡¶≤ (‡¶ï‡ßÅ‡¶ï‡¶ø‡¶ú‡ßá ‡¶∏‡¶Æ‡¶∏‡ßç‡¶Ø‡¶æ)")
            return

        soup = BeautifulSoup(r2.text, 'html.parser')
        links = set()  # ‡¶°‡ßÅ‡¶™‡ßç‡¶≤‡¶ø‡¶ï‡ßá‡¶ü ‡¶è‡¶°‡¶º‡¶æ‡¶®‡ßã‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø

        keywords = [
            "verify", "confirm", "activate", "verification", "click here", "complete setup",
            "secure your account", "action required", "email preferences", "login", "auth", "oauth",
            "click the button below", "finish setting up"
        ]

        for a in soup.find_all("a", href=True):
            href = a["href"]
            text_lower = a.get_text().lower() + href.lower()

            if any(kw in text_lower for kw in keywords):
                if href.startswith("http") and len(href) > 25:
                    links.add(href)

        # Unseen ‡¶Æ‡ßá‡¶á‡¶≤ ‡¶•‡ßá‡¶ï‡ßá‡¶ì ‡¶≤‡¶ø‡¶Ç‡¶ï ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ (‡¶Ö‡¶§‡¶ø‡¶∞‡¶ø‡¶ï‡ßç‡¶§ ‡¶∏‡ßá‡¶´‡¶ü‡¶ø)
        try:
            unseen = session.get("https://outlook.live.com/mail/inbox/unseen", cookies=cookie_dict)
            if unseen.status_code == 200:
                soup2 = BeautifulSoup(unseen.text, 'html.parser')
                for a in soup2.find_all("a", href=True):
                    href = a["href"]
                    text_lower = a.get_text().lower() + href.lower()
                    if any(kw in text_lower for kw in keywords):
                        if href.startswith("http") and len(href) > 25:
                            links.add(href)
        except:
            pass

        if links:
            msg = f"‚úÖ *{len(links)}‡¶ü‡¶æ ‡¶≠‡ßá‡¶∞‡¶ø‡¶´‡¶ø‡¶ï‡ßá‡¶∂‡¶® ‡¶≤‡¶ø‡¶Ç‡¶ï ‡¶™‡¶æ‡¶ì‡¶Ø‡¶º‡¶æ ‡¶ó‡ßá‡¶õ‡ßá!*\n\n"
            for i, link in enumerate(list(links)[:7], 1):
                short = link[:90] + "..." if len(link) > 90 else link
                msg += f"{i}. {short}\n\n"
            await update.message.reply_text(msg, parse_mode="Markdown", disable_web_page_preview=True)
        else:
            await update.message.reply_text("‚ö†Ô∏è ‡¶ï‡ßã‡¶®‡ßã ‡¶≠‡ßá‡¶∞‡¶ø‡¶´‡¶ø‡¶ï‡ßá‡¶∂‡¶® ‡¶≤‡¶ø‡¶Ç‡¶ï ‡¶™‡¶æ‡¶ì‡¶Ø‡¶º‡¶æ ‡¶Ø‡¶æ‡¶Ø‡¶º‡¶®‡¶ø (‡¶π‡¶Ø‡¶º‡¶§‡ßã ‡¶Æ‡ßá‡¶á‡¶≤ ‡¶Ü‡¶∏‡ßá‡¶®‡¶ø ‡¶¨‡¶æ ‡¶∏‡ßç‡¶™‡ßç‡¶Ø‡¶æ‡¶Æ‡ßá)")

    except requests.exceptions.Timeout:
        await update.message.reply_text("‚è∞ ‡¶ü‡¶æ‡¶á‡¶Æ‡¶Ü‡¶â‡¶ü! ‡¶ï‡ßÅ‡¶ï‡¶ø‡¶ú ‡¶†‡¶ø‡¶ï ‡¶Ü‡¶õ‡ßá ‡¶ï‡¶ø‡¶®‡ßç‡¶§‡ßÅ Microsoft ‡¶¨‡ßç‡¶≤‡¶ï ‡¶ï‡¶∞‡¶õ‡ßá‡•§ ‡ßß‡ß¶ ‡¶Æ‡¶ø‡¶®‡¶ø‡¶ü ‡¶™‡¶∞ ‡¶ü‡ßç‡¶∞‡¶æ‡¶á ‡¶ï‡¶∞‡ßã‡•§")
    except Exception as e:
        await update.message.reply_text(f"‚ùå ‡¶è‡¶∞‡¶∞: {str(e)}")


# main.py ‡¶§‡ßá ‡¶Ø‡ßã‡¶ó ‡¶ï‡¶∞‡ßã
from telegram.ext import CommandHandler, MessageHandler, filters

app.add_handler(CommandHandler("verify", verify_command))
app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, process_account))
